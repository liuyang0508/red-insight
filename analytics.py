"""
ç»Ÿè®¡åˆ†ææ¨¡å—
ç”Ÿæˆé‡åŒ–ç»Ÿè®¡æŠ¥è¡¨ã€æ•°æ®å¯è§†åŒ–ã€çƒ­è¯åˆ†æç­‰
"""
import re
import math
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass, asdict, field
from datetime import datetime
from collections import Counter
from enum import Enum


class MetricType(str, Enum):
    """æŒ‡æ ‡ç±»å‹"""
    LIKES = "likes"
    COMMENTS = "comments"
    ENGAGEMENT = "engagement"
    QUALITY = "quality"


@dataclass
class EngagementDistribution:
    """äº’åŠ¨åˆ†å¸ƒ"""
    range_label: str        # èŒƒå›´æ ‡ç­¾ å¦‚ "0-100", "100-1k"
    count: int              # å¸–å­æ•°é‡
    percentage: float       # å æ¯”
    total_engagement: int   # è¯¥èŒƒå›´æ€»äº’åŠ¨


@dataclass
class HotWord:
    """çƒ­è¯"""
    word: str
    count: int
    weight: float           # æƒé‡(åŸºäºå‡ºç°æ¬¡æ•°å’Œäº’åŠ¨é‡)
    related_posts: int      # ç›¸å…³å¸–å­æ•°


@dataclass
class AuthorStats:
    """ä½œè€…ç»Ÿè®¡"""
    author: str
    posts_count: int
    total_likes: int
    total_comments: int
    avg_engagement: float
    top_post: str


@dataclass
class TrendPoint:
    """è¶‹åŠ¿ç‚¹"""
    label: str              # æ ‡ç­¾(å¦‚æ—¶é—´ç‚¹)
    value: float
    change: float = 0       # å˜åŒ–ç‡


@dataclass
class QualityScore:
    """å†…å®¹è´¨é‡è¯„åˆ†"""
    post_id: str
    post_title: str
    total_score: float      # æ€»åˆ† (0-100)
    engagement_score: float # äº’åŠ¨åˆ†
    content_score: float    # å†…å®¹åˆ†
    author_score: float     # ä½œè€…å½±å“åŠ›åˆ†
    viral_potential: str    # ç—…æ¯’ä¼ æ’­æ½œåŠ›: low, medium, high


@dataclass
class StatisticsReport:
    """ç»Ÿè®¡æŠ¥å‘Š"""
    keyword: str
    total_posts: int
    total_likes: int
    total_comments: int
    total_engagement: int
    avg_likes: float
    avg_comments: float
    avg_engagement: float
    max_likes: int
    max_comments: int
    
    # åˆ†å¸ƒæ•°æ®
    engagement_distribution: List[EngagementDistribution]
    
    # çƒ­è¯åˆ†æ
    hot_words: List[HotWord]
    
    # æ ‡ç­¾åˆ†æ
    top_tags: List[Dict]
    
    # ä½œè€…ç»Ÿè®¡
    top_authors: List[AuthorStats]
    
    # è´¨é‡è¯„åˆ†
    quality_scores: List[QualityScore]
    
    # æ´å¯Ÿ
    insights: List[str]
    
    generated_at: str = ""
    
    def __post_init__(self):
        if not self.generated_at:
            self.generated_at = datetime.now().isoformat()


# ä¸­æ–‡åœç”¨è¯
STOP_WORDS = {
    'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª',
    'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½',
    'è‡ªå·±', 'è¿™', 'é‚£', 'é‡Œ', 'ä¸º', 'ä»€ä¹ˆ', 'å—', 'ä¸ª', 'èƒ½', 'ä¹ˆ', 'åš', 'è¢«',
    'ä¸', 'åŠ', 'ç­‰', 'ä½†', 'è¿˜', 'å¯ä»¥', 'è¿™ä¸ª', 'é‚£ä¸ª', 'æ²¡', 'æ¥', 'è®©', 'ç»™',
    'æŠŠ', 'ä»', 'æœ€', 'æ›´', 'çœŸçš„', 'è§‰å¾—', 'çœŸ', 'å¤ª', 'å•Š', 'å‘¢', 'å§', 'å˜›',
    'å‘€', 'å“¦', 'å“ˆ', 'å“ˆå“ˆ', 'å—¯', 'å“‡', 'çœŸçš„æ˜¯', 'å¤ªå¤ªå¤ª', 'è¶…çº§', 'éå¸¸',
    'ç‰¹åˆ«', 'è¶…', 'å·¨', 'ç»ç»å­', 'å®¶äººä»¬', 'å§å¦¹ä»¬', 'å®å­ä»¬', 'é›†ç¾ä»¬',
}


class AnalyticsService:
    """ç»Ÿè®¡åˆ†ææœåŠ¡"""
    
    def __init__(self):
        pass
    
    def _parse_number(self, value: str) -> int:
        """è§£ææ•°å­—"""
        if not value:
            return 0
        value = str(value).lower().strip()
        if 'w' in value or 'ä¸‡' in value:
            return int(float(value.replace('w', '').replace('ä¸‡', '')) * 10000)
        if 'k' in value or 'åƒ' in value:
            return int(float(value.replace('k', '').replace('åƒ', '')) * 1000)
        try:
            return int(''.join(filter(str.isdigit, value))) or 0
        except:
            return 0
    
    def _extract_words(self, text: str) -> List[str]:
        """æå–ä¸­æ–‡è¯æ±‡ï¼ˆç®€å•åˆ†è¯ï¼‰"""
        if not text:
            return []
        
        # æ¸…ç†æ–‡æœ¬
        text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9]', ' ', text)
        
        words = []
        
        # æå–2-4å­—çš„ä¸­æ–‡è¯ç»„
        chinese_text = ''.join(re.findall(r'[\u4e00-\u9fa5]+', text))
        for length in [4, 3, 2]:
            for i in range(len(chinese_text) - length + 1):
                word = chinese_text[i:i+length]
                if word not in STOP_WORDS:
                    words.append(word)
        
        # æå–è‹±æ–‡å•è¯
        english_words = re.findall(r'[a-zA-Z]{2,}', text)
        words.extend([w.lower() for w in english_words])
        
        return words
    
    def analyze_engagement_distribution(self, posts: List[Dict]) -> List[EngagementDistribution]:
        """åˆ†æäº’åŠ¨åˆ†å¸ƒ"""
        if not posts:
            return []
        
        # å®šä¹‰èŒƒå›´
        ranges = [
            (0, 100, "0-100"),
            (100, 500, "100-500"),
            (500, 1000, "500-1k"),
            (1000, 5000, "1k-5k"),
            (5000, 10000, "5k-1w"),
            (10000, 50000, "1w-5w"),
            (50000, float('inf'), "5w+")
        ]
        
        distribution = {r[2]: {'count': 0, 'engagement': 0} for r in ranges}
        
        for post in posts:
            engagement = (
                self._parse_number(post.get('likes', '0')) +
                self._parse_number(post.get('comments', '0'))
            )
            
            for min_val, max_val, label in ranges:
                if min_val <= engagement < max_val:
                    distribution[label]['count'] += 1
                    distribution[label]['engagement'] += engagement
                    break
        
        total = len(posts)
        result = []
        for min_val, max_val, label in ranges:
            data = distribution[label]
            if data['count'] > 0:
                result.append(EngagementDistribution(
                    range_label=label,
                    count=data['count'],
                    percentage=round(data['count'] / total * 100, 1),
                    total_engagement=data['engagement']
                ))
        
        return result
    
    def analyze_hot_words(self, posts: List[Dict], top_n: int = 20) -> List[HotWord]:
        """åˆ†æçƒ­è¯"""
        word_stats = {}  # word -> {count, engagement, posts}
        
        for post in posts:
            text = post.get('title', '') + ' ' + post.get('content', '')
            words = self._extract_words(text)
            engagement = self._parse_number(post.get('likes', '0'))
            
            seen_in_post = set()
            for word in words:
                if word not in seen_in_post:
                    seen_in_post.add(word)
                    if word not in word_stats:
                        word_stats[word] = {'count': 0, 'engagement': 0, 'posts': 0}
                    word_stats[word]['posts'] += 1
                word_stats[word]['count'] += 1
                word_stats[word]['engagement'] += engagement
        
        # è®¡ç®—æƒé‡å¹¶æ’åº
        hot_words = []
        max_count = max((s['count'] for s in word_stats.values()), default=1)
        max_engagement = max((s['engagement'] for s in word_stats.values()), default=1)
        
        for word, stats in word_stats.items():
            if stats['posts'] >= 2:  # è‡³å°‘å‡ºç°åœ¨2ç¯‡å¸–å­ä¸­
                # æƒé‡ = å‡ºç°æ¬¡æ•° * 0.4 + äº’åŠ¨é‡ * 0.6 (å½’ä¸€åŒ–)
                weight = (
                    (stats['count'] / max_count) * 0.4 +
                    (stats['engagement'] / max_engagement) * 0.6
                )
                hot_words.append(HotWord(
                    word=word,
                    count=stats['count'],
                    weight=round(weight, 3),
                    related_posts=stats['posts']
                ))
        
        hot_words.sort(key=lambda x: x.weight, reverse=True)
        return hot_words[:top_n]
    
    def analyze_tags(self, posts: List[Dict], top_n: int = 10) -> List[Dict]:
        """åˆ†ææ ‡ç­¾"""
        tag_stats = {}  # tag -> {count, engagement}
        
        for post in posts:
            tags = post.get('tags', [])
            engagement = self._parse_number(post.get('likes', '0'))
            
            for tag in tags:
                if tag and len(tag) > 1:
                    if tag not in tag_stats:
                        tag_stats[tag] = {'count': 0, 'engagement': 0}
                    tag_stats[tag]['count'] += 1
                    tag_stats[tag]['engagement'] += engagement
        
        result = [
            {'tag': tag, 'count': stats['count'], 'engagement': stats['engagement']}
            for tag, stats in tag_stats.items()
        ]
        result.sort(key=lambda x: x['engagement'], reverse=True)
        
        return result[:top_n]
    
    def analyze_authors(self, posts: List[Dict], top_n: int = 10) -> List[AuthorStats]:
        """åˆ†æä½œè€…"""
        author_data = {}  # author -> {posts, likes, comments, top_post}
        
        for post in posts:
            author = post.get('author', 'æœªçŸ¥')
            if not author or author == 'æœªçŸ¥':
                continue
            
            likes = self._parse_number(post.get('likes', '0'))
            comments = self._parse_number(post.get('comments', '0'))
            
            if author not in author_data:
                author_data[author] = {
                    'posts': [],
                    'total_likes': 0,
                    'total_comments': 0,
                    'top_post': ('', 0)
                }
            
            author_data[author]['posts'].append(post)
            author_data[author]['total_likes'] += likes
            author_data[author]['total_comments'] += comments
            
            if likes > author_data[author]['top_post'][1]:
                author_data[author]['top_post'] = (post.get('title', '')[:40], likes)
        
        result = []
        for author, data in author_data.items():
            total_engagement = data['total_likes'] + data['total_comments']
            result.append(AuthorStats(
                author=author,
                posts_count=len(data['posts']),
                total_likes=data['total_likes'],
                total_comments=data['total_comments'],
                avg_engagement=round(total_engagement / len(data['posts']), 1),
                top_post=data['top_post'][0]
            ))
        
        result.sort(key=lambda x: x.total_likes, reverse=True)
        return result[:top_n]
    
    def calculate_quality_scores(self, posts: List[Dict]) -> List[QualityScore]:
        """è®¡ç®—å†…å®¹è´¨é‡è¯„åˆ†"""
        if not posts:
            return []
        
        # æ‰¾å‡ºæœ€å¤§å€¼ç”¨äºå½’ä¸€åŒ–
        max_likes = max(self._parse_number(p.get('likes', '0')) for p in posts) or 1
        max_comments = max(self._parse_number(p.get('comments', '0')) for p in posts) or 1
        
        scores = []
        for post in posts:
            likes = self._parse_number(post.get('likes', '0'))
            comments = self._parse_number(post.get('comments', '0'))
            title = post.get('title', '')
            content = post.get('content', '')
            
            # äº’åŠ¨åˆ† (0-40)
            engagement_score = (
                (likes / max_likes) * 25 +
                (comments / max_comments) * 15
            )
            
            # å†…å®¹åˆ† (0-35)
            content_length = len(title) + len(content)
            title_quality = min(len(title) / 30, 1) * 15  # æ ‡é¢˜é•¿åº¦è¯„åˆ†
            content_quality = min(content_length / 200, 1) * 10  # å†…å®¹ä¸°å¯Œåº¦
            emoji_bonus = min(len(re.findall(r'[\U0001F300-\U0001F9FF]', title + content)) * 0.5, 5)
            hashtag_bonus = min(len(post.get('tags', [])) * 1.5, 5)
            content_score = title_quality + content_quality + emoji_bonus + hashtag_bonus
            
            # ä½œè€…å½±å“åŠ›åˆ† (0-25) - åŸºäºäº’åŠ¨ç‡ä¼°ç®—
            engagement_rate = (likes + comments * 2) / max(1, likes + 1)
            author_score = min(engagement_rate * 10, 25)
            
            total_score = engagement_score + content_score + author_score
            
            # ç—…æ¯’ä¼ æ’­æ½œåŠ›
            if total_score >= 70:
                viral_potential = "high"
            elif total_score >= 45:
                viral_potential = "medium"
            else:
                viral_potential = "low"
            
            scores.append(QualityScore(
                post_id=post.get('id', ''),
                post_title=title[:50],
                total_score=round(total_score, 1),
                engagement_score=round(engagement_score, 1),
                content_score=round(content_score, 1),
                author_score=round(author_score, 1),
                viral_potential=viral_potential
            ))
        
        scores.sort(key=lambda x: x.total_score, reverse=True)
        return scores
    
    def generate_insights(
        self, 
        posts: List[Dict],
        hot_words: List[HotWord],
        distribution: List[EngagementDistribution],
        authors: List[AuthorStats]
    ) -> List[str]:
        """ç”Ÿæˆæ•°æ®æ´å¯Ÿ"""
        insights = []
        
        if not posts:
            return ["æš‚æ— è¶³å¤Ÿæ•°æ®ç”Ÿæˆæ´å¯Ÿ"]
        
        # æ€»ä½“æ•°æ®æ´å¯Ÿ
        total_engagement = sum(
            self._parse_number(p.get('likes', '0')) + 
            self._parse_number(p.get('comments', '0'))
            for p in posts
        )
        avg_engagement = total_engagement / len(posts) if posts else 0
        
        insights.append(f"ğŸ“Š å…±åˆ†æ {len(posts)} ç¯‡å†…å®¹ï¼Œæ€»äº’åŠ¨é‡ {total_engagement:,}")
        
        if avg_engagement > 10000:
            insights.append(f"ğŸ”¥ å¹³å‡äº’åŠ¨é‡ {avg_engagement:,.0f}ï¼Œå†…å®¹çƒ­åº¦æé«˜")
        elif avg_engagement > 1000:
            insights.append(f"âœ¨ å¹³å‡äº’åŠ¨é‡ {avg_engagement:,.0f}ï¼Œå†…å®¹çƒ­åº¦è¾ƒé«˜")
        
        # çƒ­è¯æ´å¯Ÿ
        if hot_words and len(hot_words) >= 3:
            top_words = ', '.join(hw.word for hw in hot_words[:5])
            insights.append(f"ğŸ’¬ çƒ­é—¨å…³é”®è¯ï¼š{top_words}")
        
        # åˆ†å¸ƒæ´å¯Ÿ
        if distribution:
            high_engagement = [d for d in distribution if '1w' in d.range_label or '5w' in d.range_label]
            if high_engagement:
                high_count = sum(d.count for d in high_engagement)
                high_pct = sum(d.percentage for d in high_engagement)
                if high_pct > 20:
                    insights.append(f"ğŸš€ {high_pct:.1f}% çš„å†…å®¹äº’åŠ¨é‡è¿‡ä¸‡ï¼Œçˆ†æ¬¾ç‡è¾ƒé«˜")
        
        # ä½œè€…æ´å¯Ÿ
        if authors and len(authors) >= 1:
            top_author = authors[0]
            insights.append(f"ğŸ‘‘ å¤´éƒ¨åˆ›ä½œè€…ã€Œ{top_author.author}ã€è´¡çŒ®äº† {top_author.total_likes:,} ç‚¹èµ")
        
        # äº’åŠ¨æ¯”ä¾‹æ´å¯Ÿ
        total_likes = sum(self._parse_number(p.get('likes', '0')) for p in posts)
        total_comments = sum(self._parse_number(p.get('comments', '0')) for p in posts)
        if total_likes > 0:
            comment_rate = total_comments / total_likes
            if comment_rate > 0.1:
                insights.append(f"ğŸ’­ è¯„è®ºç‡ {comment_rate*100:.1f}%ï¼Œç”¨æˆ·è®¨è®ºçƒ­åº¦é«˜")
        
        return insights
    
    def generate_report(self, posts: List[Dict], keyword: str = "") -> StatisticsReport:
        """ç”Ÿæˆå®Œæ•´ç»Ÿè®¡æŠ¥å‘Š"""
        if not posts:
            return StatisticsReport(
                keyword=keyword,
                total_posts=0,
                total_likes=0,
                total_comments=0,
                total_engagement=0,
                avg_likes=0,
                avg_comments=0,
                avg_engagement=0,
                max_likes=0,
                max_comments=0,
                engagement_distribution=[],
                hot_words=[],
                top_tags=[],
                top_authors=[],
                quality_scores=[],
                insights=["æš‚æ— æ•°æ®"]
            )
        
        # åŸºç¡€ç»Ÿè®¡
        likes_list = [self._parse_number(p.get('likes', '0')) for p in posts]
        comments_list = [self._parse_number(p.get('comments', '0')) for p in posts]
        
        total_likes = sum(likes_list)
        total_comments = sum(comments_list)
        total_engagement = total_likes + total_comments
        
        # åˆ†æå„é¡¹
        distribution = self.analyze_engagement_distribution(posts)
        hot_words = self.analyze_hot_words(posts)
        tags = self.analyze_tags(posts)
        authors = self.analyze_authors(posts)
        quality_scores = self.calculate_quality_scores(posts)
        insights = self.generate_insights(posts, hot_words, distribution, authors)
        
        return StatisticsReport(
            keyword=keyword,
            total_posts=len(posts),
            total_likes=total_likes,
            total_comments=total_comments,
            total_engagement=total_engagement,
            avg_likes=round(total_likes / len(posts), 1),
            avg_comments=round(total_comments / len(posts), 1),
            avg_engagement=round(total_engagement / len(posts), 1),
            max_likes=max(likes_list),
            max_comments=max(comments_list),
            engagement_distribution=distribution,
            hot_words=hot_words,
            top_tags=tags,
            top_authors=authors,
            quality_scores=quality_scores,
            insights=insights
        )
    
    def compare_keywords(
        self, 
        keyword_posts: Dict[str, List[Dict]]
    ) -> Dict:
        """å¯¹æ¯”å¤šä¸ªå…³é”®è¯çš„æ•°æ®"""
        comparison = {
            "keywords": [],
            "comparison_chart": [],
            "winner": None,
            "insights": []
        }
        
        keyword_stats = []
        for keyword, posts in keyword_posts.items():
            total_likes = sum(self._parse_number(p.get('likes', '0')) for p in posts)
            total_comments = sum(self._parse_number(p.get('comments', '0')) for p in posts)
            
            stat = {
                "keyword": keyword,
                "posts_count": len(posts),
                "total_likes": total_likes,
                "total_comments": total_comments,
                "total_engagement": total_likes + total_comments,
                "avg_engagement": round((total_likes + total_comments) / len(posts), 1) if posts else 0
            }
            keyword_stats.append(stat)
            comparison["keywords"].append(keyword)
        
        # æ’åºæ‰¾å‡ºèµ¢å®¶
        keyword_stats.sort(key=lambda x: x['total_engagement'], reverse=True)
        comparison["comparison_chart"] = keyword_stats
        
        if keyword_stats:
            comparison["winner"] = keyword_stats[0]["keyword"]
            
            # ç”Ÿæˆå¯¹æ¯”æ´å¯Ÿ
            if len(keyword_stats) >= 2:
                first = keyword_stats[0]
                second = keyword_stats[1]
                ratio = first['total_engagement'] / max(second['total_engagement'], 1)
                comparison["insights"].append(
                    f"ã€Œ{first['keyword']}ã€çƒ­åº¦é¢†å…ˆï¼Œæ˜¯ã€Œ{second['keyword']}ã€çš„ {ratio:.1f} å€"
                )
        
        return comparison


def report_to_dict(report: StatisticsReport) -> Dict:
    """è½¬æ¢æŠ¥å‘Šä¸ºå­—å…¸ï¼ˆç”¨äºJSONåºåˆ—åŒ–ï¼‰"""
    return {
        "keyword": report.keyword,
        "total_posts": report.total_posts,
        "total_likes": report.total_likes,
        "total_comments": report.total_comments,
        "total_engagement": report.total_engagement,
        "avg_likes": report.avg_likes,
        "avg_comments": report.avg_comments,
        "avg_engagement": report.avg_engagement,
        "max_likes": report.max_likes,
        "max_comments": report.max_comments,
        "engagement_distribution": [asdict(d) for d in report.engagement_distribution],
        "hot_words": [asdict(hw) for hw in report.hot_words],
        "top_tags": report.top_tags,
        "top_authors": [asdict(a) for a in report.top_authors],
        "quality_scores": [asdict(q) for q in report.quality_scores],
        "insights": report.insights,
        "generated_at": report.generated_at
    }


async def main():
    """æµ‹è¯•ç»Ÿè®¡åˆ†æ"""
    # æ¨¡æ‹Ÿæ•°æ®
    sample_posts = [
        {"id": "1", "title": "è¶…å¥½ç”¨çš„æŠ¤è‚¤å“æ¨èï¼æ•æ„Ÿè‚Œå¿…å…¥ ğŸŒŸ", "content": "åˆ†äº«æˆ‘çš„æŠ¤è‚¤å¿ƒå¾—...", "author": "ç¾å¦†è¾¾äºº", "likes": "2.3w", "comments": "1234", "tags": ["æŠ¤è‚¤", "æ•æ„Ÿè‚Œ"]},
        {"id": "2", "title": "è¿™æ¬¾é¢éœœçœŸçš„ç»äº†ï¼å¹³ä»·å¥½ç”¨", "content": "ç”¨äº†ä¸€ä¸ªæœˆæ•ˆæœè¶…æ£’...", "author": "å°çº¢è–¯", "likes": "8500", "comments": "432", "tags": ["æŠ¤è‚¤", "é¢éœœ"]},
        {"id": "3", "title": "æŠ¤è‚¤æ–°æ‰‹å…¥é—¨æŒ‡å—", "content": "åˆšå¼€å§‹æŠ¤è‚¤çš„å§å¦¹çœ‹è¿‡æ¥...", "author": "æŠ¤è‚¤å°è¯¾å ‚", "likes": "1.5w", "comments": "876", "tags": ["æŠ¤è‚¤", "æ–°æ‰‹"]},
    ]
    
    service = AnalyticsService()
    report = service.generate_report(sample_posts, "æŠ¤è‚¤å“")
    
    print("ğŸ“Š ç»Ÿè®¡æŠ¥å‘Š")
    print("=" * 50)
    print(f"å…³é”®è¯: {report.keyword}")
    print(f"å¸–å­æ•°: {report.total_posts}")
    print(f"æ€»ç‚¹èµ: {report.total_likes:,}")
    print(f"æ€»è¯„è®º: {report.total_comments:,}")
    print(f"å¹³å‡äº’åŠ¨: {report.avg_engagement:,.1f}")
    
    print("\nğŸ”¥ çƒ­è¯:")
    for hw in report.hot_words[:5]:
        print(f"  â€¢ {hw.word}: {hw.count}æ¬¡, æƒé‡ {hw.weight}")
    
    print("\nğŸ“ˆ æ´å¯Ÿ:")
    for insight in report.insights:
        print(f"  {insight}")


if __name__ == "__main__":
    import asyncio
    asyncio.run(main())

