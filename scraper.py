"""
å°çº¢ä¹¦å¸–å­æŠ“å–æ¨¡å—
ä½¿ç”¨ Playwright è¿›è¡Œç½‘é¡µæŠ“å–
"""
import asyncio
import re
import json
from typing import List, Optional
from dataclasses import dataclass, asdict
from datetime import datetime
from urllib.parse import quote


@dataclass
class RedBookPost:
    """å°çº¢ä¹¦å¸–å­æ•°æ®æ¨¡å‹"""
    id: str
    title: str
    content: str
    author: str
    author_avatar: str
    likes: str
    comments: str
    cover_image: str
    url: str
    tags: List[str]
    scraped_at: str


class RedBookScraper:
    """å°çº¢ä¹¦çˆ¬è™«ç±»"""
    
    # å°çº¢ä¹¦ç™»å½• Cookie
    COOKIES = [
        {"name": "abRequestId", "value": "600fe684-1927-5084-b0bc-ddae02b6599d", "domain": ".xiaohongshu.com", "path": "/"},
        {"name": "a1", "value": "198c80d769epzge5avncl1qtwr8ptwq0dplota3a630000448921", "domain": ".xiaohongshu.com", "path": "/"},
        {"name": "webId", "value": "0dd2a2d89b6955ff9298b027ddd96b15", "domain": ".xiaohongshu.com", "path": "/"},
        {"name": "gid", "value": "yjYSY8fjJ8fdyjYSY8fWKU0qjdUukd20hIS110KyA9EFYlq8U9EA8988844YjJy8qijd08dS", "domain": ".xiaohongshu.com", "path": "/"},
        {"name": "webBuild", "value": "5.6.5", "domain": ".xiaohongshu.com", "path": "/"},
        {"name": "acw_tc", "value": "0a4a9a7a17677935792511602ed8d03daed6d0205019cce58fd9a885ef5c21", "domain": ".xiaohongshu.com", "path": "/"},
        {"name": "web_session", "value": "040069b204a2de2fa246d52b6b3b4be441e85c", "domain": ".xiaohongshu.com", "path": "/"},
        {"name": "xsecappid", "value": "xhs-pc-web", "domain": ".xiaohongshu.com", "path": "/"},
        {"name": "websectiga", "value": "82e85efc5500b609ac1166aaf086ff8aa4261153a448ef0be5b17417e4512f28", "domain": ".xiaohongshu.com", "path": "/"},
        {"name": "sec_poison_id", "value": "ac93bd2f-c85a-4c4a-bccf-3823f584ca70", "domain": ".xiaohongshu.com", "path": "/"},
    ]
    
    def __init__(self):
        self.browser = None
        self.context = None
        self.page = None
        self.playwright = None
    
    async def init_browser(self):
        """åˆå§‹åŒ–æµè§ˆå™¨"""
        from playwright.async_api import async_playwright
        
        self.playwright = await async_playwright().start()
        self.browser = await self.playwright.chromium.launch(
            headless=True,
            args=[
                '--disable-blink-features=AutomationControlled',
                '--no-sandbox',
                '--disable-dev-shm-usage',
                '--disable-web-security'
            ]
        )
        
        # åˆ›å»ºæµè§ˆå™¨ä¸Šä¸‹æ–‡
        self.context = await self.browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            locale='zh-CN'
        )
        
        # å¦‚æœæœ‰ Cookieï¼Œæ·»åŠ åˆ°ä¸Šä¸‹æ–‡
        if self.COOKIES:
            await self.context.add_cookies(self.COOKIES)
        
        self.page = await self.context.new_page()
        
        # æ³¨å…¥è„šæœ¬ç»‘è¿‡æ£€æµ‹
        await self.page.add_init_script("""
            Object.defineProperty(navigator, 'webdriver', { get: () => undefined });
            Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5] });
        """)
    
    async def close_browser(self):
        """å…³é—­æµè§ˆå™¨"""
        try:
            if self.browser:
                await self.browser.close()
            if self.playwright:
                await self.playwright.stop()
        except:
            pass
    
    async def search_posts(self, keyword: str, max_posts: int = 5) -> List[RedBookPost]:
        """æœç´¢å°çº¢ä¹¦å¸–å­"""
        posts = []
        
        try:
            await self.init_browser()
            
            # è®¿é—®å°çº¢ä¹¦æœç´¢é¡µé¢
            encoded_keyword = quote(keyword)
            search_url = f'https://www.xiaohongshu.com/search_result?keyword={encoded_keyword}&source=web_search_result_notes'
            
            print(f"ğŸ” æ­£åœ¨è®¿é—®: {search_url}")
            
            await self.page.goto(search_url, wait_until='domcontentloaded', timeout=30000)
            await asyncio.sleep(3)
            
            # å°è¯•å¤šç§é€‰æ‹©å™¨
            selectors = [
                'section.note-item',
                '[class*="note-item"]',
                '[class*="noteItem"]', 
                'a[href*="/explore/"]',
                '[class*="feeds"] [class*="note"]',
                '.search-result-item'
            ]
            
            cards = []
            for selector in selectors:
                try:
                    cards = await self.page.query_selector_all(selector)
                    if cards and len(cards) > 0:
                        print(f"âœ… ä½¿ç”¨é€‰æ‹©å™¨ '{selector}' æ‰¾åˆ° {len(cards)} ä¸ªå…ƒç´ ")
                        break
                except:
                    continue
            
            if cards:
                for i, card in enumerate(cards[:max_posts]):
                    try:
                        post = await self._extract_post_info(card, i, keyword)
                        if post:
                            posts.append(post)
                            print(f"  ğŸ“ æŠ“å–å¸–å­ {i+1}: {post.title[:30]}...")
                    except Exception as e:
                        print(f"  âŒ æå–å¸–å­ {i+1} å¤±è´¥: {e}")
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å¡ç‰‡ï¼Œå°è¯•ä»é¡µé¢ JSON æ•°æ®æå–
            if not posts:
                print("ğŸ”„ å°è¯•ä»é¡µé¢æ•°æ®æå–...")
                posts = await self._extract_from_page_json(keyword)
            
        except Exception as e:
            print(f"âŒ æŠ“å–å‡ºé”™: {e}")
        finally:
            await self.close_browser()
        
        # å¦‚æœä»ç„¶æ²¡æœ‰å†…å®¹ï¼Œè¿”å›åŸºäºå…³é”®è¯çš„æ¨¡æ‹ŸçœŸå®æ•°æ®
        if not posts:
            print(f"âš ï¸ æ— æ³•æŠ“å–çœŸå®æ•°æ®ï¼Œå°çº¢ä¹¦åçˆ¬é™åˆ¶ã€‚è¯·é…ç½®ç™»å½• Cookie è·å–çœŸå®å†…å®¹ã€‚")
            posts = self._get_demo_posts(keyword)
        
        return posts[:max_posts]
    
    async def _extract_post_info(self, card, index: int, keyword: str) -> Optional[RedBookPost]:
        """ä»å¡ç‰‡æå–å¸–å­ä¿¡æ¯"""
        try:
            # æå–é“¾æ¥
            link = await card.get_attribute('href')
            if not link:
                link_elem = await card.query_selector('a')
                if link_elem:
                    link = await link_elem.get_attribute('href')
            
            if link and not link.startswith('http'):
                link = f"https://www.xiaohongshu.com{link}"
            
            # æå–å¸–å­ ID
            post_id = ""
            if link:
                id_match = re.search(r'/explore/([a-zA-Z0-9]+)', link)
                if id_match:
                    post_id = id_match.group(1)
            
            # æå–æ ‡é¢˜
            title = ""
            title_selectors = ['[class*="title"]', 'span', 'p', '[class*="desc"]']
            for sel in title_selectors:
                try:
                    title_elem = await card.query_selector(sel)
                    if title_elem:
                        title = await title_elem.inner_text()
                        if title and len(title) > 5:
                            break
                except:
                    continue
            
            # æå–ä½œè€…
            author = ""
            author_selectors = ['[class*="author"]', '[class*="name"]', '[class*="nickname"]']
            for sel in author_selectors:
                try:
                    author_elem = await card.query_selector(sel)
                    if author_elem:
                        author = await author_elem.inner_text()
                        if author:
                            break
                except:
                    continue
            
            # æå–ç‚¹èµæ•°
            likes = "0"
            likes_selectors = ['[class*="like"]', '[class*="count"]']
            for sel in likes_selectors:
                try:
                    likes_elem = await card.query_selector(sel)
                    if likes_elem:
                        likes = await likes_elem.inner_text()
                        if likes:
                            break
                except:
                    continue
            
            # æå–å°é¢å›¾
            cover = ""
            try:
                img_elem = await card.query_selector('img')
                if img_elem:
                    cover = await img_elem.get_attribute('src') or ""
            except:
                pass
            
            if not title:
                title = f"{keyword}ç›¸å…³å†…å®¹ {index + 1}"
            
            return RedBookPost(
                id=post_id or f"post_{index}_{int(datetime.now().timestamp())}",
                title=title.strip()[:100],
                content="",
                author=author.strip() if author else "å°çº¢ä¹¦ç”¨æˆ·",
                author_avatar="",
                likes=likes.strip() if likes else "0",
                comments="0",
                cover_image=cover,
                url=link or "",
                tags=[keyword],
                scraped_at=datetime.now().isoformat()
            )
            
        except Exception as e:
            print(f"æå–å¤±è´¥: {e}")
            return None
    
    async def _extract_from_page_json(self, keyword: str) -> List[RedBookPost]:
        """ä»é¡µé¢åµŒå…¥çš„ JSON æ•°æ®æå–"""
        posts = []
        try:
            page_content = await self.page.content()
            
            # æŸ¥æ‰¾ __INITIAL_STATE__ æ•°æ®
            patterns = [
                r'window\.__INITIAL_STATE__\s*=\s*(\{.*?\});',
                r'<script[^>]*>.*?window\.__INITIAL_STATE__\s*=\s*(\{.*?\}).*?</script>',
            ]
            
            for pattern in patterns:
                match = re.search(pattern, page_content, re.DOTALL)
                if match:
                    try:
                        # æ¸…ç† JSON å­—ç¬¦ä¸²
                        json_str = match.group(1)
                        json_str = re.sub(r'undefined', 'null', json_str)
                        data = json.loads(json_str)
                        
                        # å°è¯•ä¸åŒçš„æ•°æ®è·¯å¾„
                        notes = None
                        if 'search' in data and 'notes' in data['search']:
                            notes = data['search']['notes']
                        elif 'note' in data and 'noteDetailMap' in data['note']:
                            notes = list(data['note']['noteDetailMap'].values())
                        
                        if notes:
                            for i, note in enumerate(notes[:5]):
                                note_data = note.get('note', note)
                                posts.append(RedBookPost(
                                    id=note_data.get('noteId', note_data.get('id', f'note_{i}')),
                                    title=note_data.get('title', note_data.get('displayTitle', '')),
                                    content=note_data.get('desc', '')[:200],
                                    author=note_data.get('user', {}).get('nickname', note_data.get('nickname', '')),
                                    author_avatar=note_data.get('user', {}).get('avatar', ''),
                                    likes=str(note_data.get('likedCount', note_data.get('likes', 0))),
                                    comments=str(note_data.get('commentsCount', note_data.get('comments', 0))),
                                    cover_image=note_data.get('cover', {}).get('url', note_data.get('imageList', [{}])[0].get('url', '') if note_data.get('imageList') else ''),
                                    url=f"https://www.xiaohongshu.com/explore/{note_data.get('noteId', note_data.get('id', ''))}",
                                    tags=note_data.get('tagList', [keyword]),
                                    scraped_at=datetime.now().isoformat()
                                ))
                            if posts:
                                print(f"âœ… ä»é¡µé¢æ•°æ®æå–åˆ° {len(posts)} æ¡å¸–å­")
                                break
                    except json.JSONDecodeError:
                        continue
        except Exception as e:
            print(f"ä» JSON æå–å¤±è´¥: {e}")
        
        return posts
    
    def _get_demo_posts(self, keyword: str) -> List[RedBookPost]:
        """è¿”å›æ¨¡æ‹Ÿæ•°æ®ï¼ˆå½“æ— æ³•æŠ“å–çœŸå®æ•°æ®æ—¶ï¼‰"""
        # æ³¨æ„ï¼šè¿™äº›æ˜¯æ¼”ç¤ºæ•°æ®ï¼ŒURL æ˜¯è™šæ„çš„
        # å‰ç«¯ä¼šè‡ªåŠ¨ç”Ÿæˆæ¸å˜è‰²å ä½å›¾
        demo_posts = [
            RedBookPost(
                id="demo_1",
                title=f"ã€è¶…è¯¦ç»†ã€‘{keyword}ä¿å§†çº§æ”»ç•¥åˆ†äº« ğŸ¯",
                content=f"åˆ†äº«ä¸€ä¸‹æˆ‘å…³äº{keyword}çš„å¿ƒå¾—ä½“ä¼šï¼ç»è¿‡å¤šæ¬¡å°è¯•æ€»ç»“å‡ºæ¥çš„ç»éªŒï¼Œå¸Œæœ›å¯¹å¤§å®¶æœ‰å¸®åŠ©~",
                author="ç”Ÿæ´»å°è¾¾äºº",
                author_avatar="",
                likes="2.3w",
                comments="1856",
                cover_image="",  # å‰ç«¯ä¼šæ˜¾ç¤ºæ¸å˜è‰²å ä½
                url=f"https://www.xiaohongshu.com/search_result?keyword={quote(keyword)}",
                tags=[keyword, "å¹²è´§åˆ†äº«"],
                scraped_at=datetime.now().isoformat()
            ),
            RedBookPost(
                id="demo_2",
                title=f"{keyword}è¿™æ ·åšæ‰å¯¹ï¼äº²æµ‹æœ‰æ•ˆ âœ¨",
                content=f"å…³äº{keyword}ï¼Œæˆ‘èµ°è¿‡å¾ˆå¤šå¼¯è·¯ï¼Œä»Šå¤©æ¥åˆ†äº«æ­£ç¡®çš„æ–¹æ³•ï¼",
                author="æ—¶å°šåšä¸»å°ç¾",
                author_avatar="",
                likes="1.8w",
                comments="923",
                cover_image="",
                url=f"https://www.xiaohongshu.com/search_result?keyword={quote(keyword)}",
                tags=[keyword, "äº²æµ‹æœ‰æ•ˆ"],
                scraped_at=datetime.now().isoformat()
            ),
            RedBookPost(
                id="demo_3",
                title=f"æ–°æ‰‹å¿…çœ‹ï¼{keyword}å…¥é—¨å…¨æ”»ç•¥ ğŸ“š",
                content=f"æ–°æ‰‹å¦‚ä½•å¿«é€Ÿå…¥é—¨{keyword}ï¼Ÿè¿™ç¯‡æ–‡ç« å¸®ä½ è§£ç­”æ‰€æœ‰ç–‘é—®ï¼",
                author="çŸ¥è¯†åˆ†äº«å®˜",
                author_avatar="",
                likes="5.6w",
                comments="2341",
                cover_image="",
                url=f"https://www.xiaohongshu.com/search_result?keyword={quote(keyword)}",
                tags=[keyword, "æ–°æ‰‹å…¥é—¨"],
                scraped_at=datetime.now().isoformat()
            ),
            RedBookPost(
                id="demo_4",
                title=f"çœŸå®æµ‹è¯„ | {keyword}æ·±åº¦ä½“éªŒæŠ¥å‘Š ğŸ’¯",
                content=f"ä½¿ç”¨{keyword}ä¸€ä¸ªæœˆåçš„çœŸå®æ„Ÿå—åˆ†äº«~",
                author="æµ‹è¯„è¾¾äººMax",
                author_avatar="",
                likes="3.2w",
                comments="1567",
                cover_image="",
                url=f"https://www.xiaohongshu.com/search_result?keyword={quote(keyword)}",
                tags=[keyword, "çœŸå®æµ‹è¯„"],
                scraped_at=datetime.now().isoformat()
            ),
            RedBookPost(
                id="demo_5",
                title=f"2026æœ€æ–°ï¼{keyword}è¶‹åŠ¿è§£è¯» ğŸ”¥",
                content=f"ä»Šå¹´{keyword}é¢†åŸŸæœ‰å“ªäº›æ–°è¶‹åŠ¿ï¼Ÿä¸€æ–‡å¸¦ä½ äº†è§£æœ€æ–°åŠ¨æ€ï¼",
                author="è¡Œä¸šè§‚å¯Ÿè€…",
                author_avatar="",
                likes="4.1w",
                comments="1892",
                cover_image="",
                url=f"https://www.xiaohongshu.com/search_result?keyword={quote(keyword)}",
                tags=[keyword, "è¶‹åŠ¿"],
                scraped_at=datetime.now().isoformat()
            )
        ]
        return demo_posts


def posts_to_dict(posts: List[RedBookPost]) -> List[dict]:
    """å°†å¸–å­åˆ—è¡¨è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨"""
    return [asdict(post) for post in posts]


async def main():
    """æµ‹è¯•å‡½æ•°"""
    scraper = RedBookScraper()
    posts = await scraper.search_posts("å’–å•¡æ¢åº—", max_posts=5)
    
    for post in posts:
        print(f"\næ ‡é¢˜: {post.title}")
        print(f"ä½œè€…: {post.author}")
        print(f"ç‚¹èµ: {post.likes}")
        print(f"é“¾æ¥: {post.url}")


if __name__ == "__main__":
    asyncio.run(main())
